def main(h,e:int,g:int,path,dloc=''):
    # -*- coding: utf-8 -*-
    """01_TimeGAN_TF2.ipynb
    
    Automatically generated by Colaboratory.
    
    Original file is located at
        https://colab.research.google.com/drive/170nlGl_CSqiCMllV6uJ0xgUfNJgqrGhf
    
    <font size="+3">Time-series Generative Adversarial Network (TimeGAN)</font>
    
    # Introduction
    
    The material is based on the 2<sup>nd</sup> edition of my book on [Machine Learning for Trading]((https://www.amazon.com/Machine-Learning-Algorithmic-Trading-alternative/dp/1839217715?pf_rd_r=GZH2XZ35GB3BET09PCCA&pf_rd_p=c5b6893a-24f2-4a59-9d4b-aff5065c90ec&pd_rd_r=91a679c7-f069-4a6e-bdbb-a2b3f548f0c8&pd_rd_w=2B0Q0&pd_rd_wg=GMY5S&ref_=pd_gw_ci_mcx_mr_hp_d)) (see [GitHub repo](https://github.com/stefan-jansen/machine-learning-for-trading)).
    
    Generating synthetic time-series data poses specific challenges above and beyond those encountered when designing GANs for images. In addition to the distribution over variables at any given point, such as pixel values or the prices of numerous stocks, a generative model for time-series data should also learn the temporal dynamics that shapes how one sequence of observations follows another (see also discussion in [Chapter 9: Time Series Models for Volatility Forecasts and Statistical Arbitrage](https://github.com/stefan-jansen/machine-learning-for-trading/tree/master/09_time_series_models)).
    
    Very recent and promising [research]((https://proceedings.neurips.cc/paper/2019/file/c9efe5f26cd17ba6216bbe2a7d26d490-Paper.pdf)) by Yoon, Jarrett, and van der Schaar, presented at NeurIPS in December 2019, introduces a novel Time-Series Generative Adversarial Network (**TimeGAN**) framework that aims to account for temporal correlations by combining supervised and unsupervised training. The model learns a time-series embedding space while optimizing both supervised and adversarial objectives that encourage it to adhere to the dynamics observed while sampling from historical data during training. The authors test the model on various time series, including historical stock prices, and find that the quality of the synthetic data significantly outperforms that of available alternatives.
    
    Adapted from the excellent paper by Jinsung Yoon, Daniel Jarrett, and Mihaela van der Schaar:  
    [Time-series Generative Adversarial Networks](https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks),  
    Neural Information Processing Systems (NeurIPS), 2019.
    
    - Last updated Date: April 24th 2020
    - [Original code](https://bitbucket.org/mvdschaar/mlforhealthlabpub/src/master/alg/timegan/) author: Jinsung Yoon (jsyoon0823@gmail.com)
    
    We outline how this sophisticated model works, highlight key implementation steps that build on the [DCGAN](https://github.com/stefan-jansen/machine-learning-for-trading/blob/master/21_gans_for_synthetic_time_series/01_deep_convolutional_generative_adversarial_network.ipynb) example, and show how to evaluate the quality of the resulting time series. Please see the paper for additional information.
    
    # Imports & Settings
    """
    
    import warnings
    warnings.filterwarnings('ignore')
    
    import pandas as pd
    import numpy as np
    from sklearn.preprocessing import MinMaxScaler
    import tensorflow as tf
    from pathlib import Path
    from tqdm import tqdm
    
    from tensorflow.keras.models import Sequential, Model
    from tensorflow.keras.layers import GRU, Dense, RNN, GRUCell, Input, LSTM, Bidirectional, Activation, Conv1D
    from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import TensorBoard
    from tensorflow.keras.utils import plot_model
    
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    gpu_devices = tf.config.experimental.list_physical_devices('GPU')
    if gpu_devices:
        print('Using GPU')
        tf.config.experimental.set_memory_growth(gpu_devices[0], True)
    else:
        print('Using CPU')
    
    sns.set_style('white')
    
    np.random.seed(42)
    tf.random.set_seed(1234)
    
    """## Experiment Path"""
    
    results_path = Path('time_gan')
    if not results_path.exists():
        results_path.mkdir()
    
    experiment = 0
    
    log_dir = results_path / f'experiment_{experiment:02}'
    if not log_dir.exists():
        log_dir.mkdir(parents=True)
    
    hdf_store = results_path / 'TimeSeriesGAN.h5'
    
    """# TimeGAN Architecture and Training
    
    ## Learning the data generation process across features and time
    A successful generative model for time-series data needs to capture both the cross-sectional distribution of features at each point in time and the longitudinal relationships among these features over time. Expressed in the image context we just discussed, the model needs to learn not only what a realistic image looks like, but also how one image evolves from the next as in a video.
    
    ## Combining adversarial and supervised training with time-series embedding
    
    Prior attempts at generating time-series data like the recurrent (conditional) GAN relied on recurrent neural networks (RNN, see [Chapter 19, RNN for Multivariate Time Series and Sentiment Analysis](https://github.com/stefan-jansen/machine-learning-for-trading/tree/master/19_recurrent_neural_nets)) in the roles of generator and discriminator. TimeGAN explicitly incorporates the autoregressive nature of time series by combining the unsupervised adversarial loss on both real and synthetic sequences familiar from the DCGAN example with a stepwise supervised loss with respect to the original data. The goal is to reward the model for learning the distribution over transitions from one point in time to the next present in the historical data.
    
    Furthermore, TimeGAN includes an embedding network that maps the time-series features to a lower-dimensional latent space to reduce the complexity of the adversarial space. The motivation is to capture the drivers of temporal dynamics that often have lower dimensionality (see also the discussions of manifold learning in [Chapter 13](https://github.com/stefan-jansen/machine-learning-for-trading/tree/master/13_unsupervised_learning) and nonlinear dimensionality reduction in [Chapter 20](https://github.com/stefan-jansen/machine-learning-for-trading/tree/master/20_autoencoders_for_conditional_risk_factors)). 
    
    A **key element** of the TimeGAN architecture is that both the generator and the embedding (or autoencoder) network are responsible for minimizing the supervised loss that measures how well the model learns the dynamic relationship. 
    
    As a result, the model learns a latent space conditioned on facilitating the generator’s task to faithfully reproduce the temporal relationships observed in the historical data. In addition to time-series data, the model can also process *static* data that do not change or change less frequently over time.
    
    The design of the TimeGAN components follows the author's sample code.
    
    ## The four components of the TimeGAN architecture
    The TimeGAN architecture combines an adversarial network with an autoencoder and has thus **four network components** as depicted in the below Figure:
    - **Autoencoder**: embedding and recovery networks
    - **Adversarial Network**: sequence generator and sequence discriminator components
    
    <img src="https://i.imgur.com/2QurkKl.png" title="source: imgur.com" width="600"/>
    
    The **embedding and recovery components** of the autoencoder map the feature space into the latent space and vice versa to facilitate the learning of the temporal dynamics by the adversarial network learns in a lower-dimensional space. The authors implement the embedding and recovery network using a stacked RNN and a feedforward network. However, these choices can be flexibly adapted to the task at hand as long as they are autoregressive and respect the temporal order of the data.
    
    The **generator and discriminator elements** of the adversarial network differ from the DCGAN not only because they operate on sequential data but also because the synthetic features are generated in the latent space that the model learns simultaneously. The authors choose an RNN as generator and a bidirectional RNN with a feedforward output layer for the discriminator.
    
    ## Joint training of autoencoder and adversarial network
    The three loss functions displayed in the Figure above drive the joint optimization of the network elements just described while training on real and randomly generated time series. In more detail, they aim to accomplish the following:
    - The **reconstruction loss** is familiar from our discussion of autoencoders in [Chapter 20](https://github.com/stefan-jansen/machine-learning-for-trading/tree/master/20_autoencoders_for_conditional_risk_factors); it compares how well the reconstruction of the encoded data resembles the original. 
    - The **unsupervised loss** reflects the competitive interaction between the generator and the discriminator described in the [DCGAN](https://github.com/stefan-jansen/machine-learning-for-trading/blob/master/21_gans_for_synthetic_time_series/01_deep_convolutional_generative_adversarial_network.ipynb) example; while the generator aims to minimize the probability that the discriminator classifies its output as fake, the discriminator aims to optimize the correct classification or real and fake inputs.
    - The **supervised loss** captures how well the generator approximates the actual next time step in latent space when receiving encoded real data for the prior sequence.
    
    As a result of the joint training, TimeGAN simultaneously learns to encode features, generate representations, and iterate across time. More specifically, the embedding network creates the latent space, the adversarial network operates within this space, and supervised loss synchronizes the latent dynamics of both real and synthetic data.
    
    ## Training in three phases
    
    Training proceeds along the following three phases:
    1. Train the autoencoder on real time series to optimize reconstruction
    2. Optimize the supervised loss using real time series to capture the temporal dynamics of the historical data
    3. Jointly train the four components while minimizing all three loss functions
    
    TimeGAN includes several hyperparameters used to weigh the components of composite loss functions; however, the authors find the network to be less sensitive to these settings than one might expect given the notorious difficulties of GAN training. In fact, they do not discover significant challenges during training and suggest that the embedding task serves to regularize adversarial learning because it reduces its dimensionality while the supervised loss constrains the stepwise dynamics of the generator.
    
    ## TimeGAN with TensorFlow 2
    
    We now turn to the TimeGAN implementation using TensorFlow 2; see the paper for an in-depth explanation of the math and methodology of the approach.
    
    In the next sections, we'll implement the TimeGAN architecture just described. The authors provide sample code using TensorFlow 1 that we port to TensorFlow 2. Building and training TimeGAN requires several steps:
    1. Selecting and preparing real and random time series inputs
    2. Creating the key TimeGAN model components
    3. Defining the various loss functions and train steps used during the three training phases
    4. Running the training loops and logging the results
    5. Generating synthetic time series and evaluating the results
    
    We’ll walk through the key items for each of these steps; please refer to the notebook TimeGAN_TF2 for the code examples in this section (unless otherwise noted) as well as additional implementation details.
    
    # Prepare Data
    
    The authors demonstrate the applicability of TimeGAN to financial data using 15 years of daily Google stock prices downloaded from Yahoo Finance with six features, namely open, high, low, close and adjusted close price, and volume. We’ll use instead close to 20 years of adjusted close prices for six different tickers because it introduces somewhat higher variability. We follow the original paper in targeting synthetic series with 24 time steps.
    """
    
    df = pd.read_csv(dloc+'stockss.csv', 
                     index_col='Date', 
                     parse_dates=['Date'])
    df.info()
    
    
    """## Parameters"""
    
    seq_len = 24
    n_seq = 16
    batch_size = 128
    
    tickers = list(df.columns)#['SP', 'Gold', 'Oil', 'TBF', 'NASDAQ', 'APLL']
    
    """## Plot Series
    
    Among the stocks with the longest history in the Quandl Wiki dataset are those displayed in normalized format, i.e., starting at 1.0, in the following Figure 21.5. We retrieve the adjusted close from 2000-2017 and obtain over 4,000 observations. The correlation coefficient among the series ranges from 0.01 for GE and CAT. to 0.94 for DIS and KO.
    """
    
    axes = df.div(df.iloc[0]).plot(subplots=True,
                                   figsize=(14, 17),
                                   layout=(8, 2),
                                   title=tickers,
                                   legend=False,
                                   rot=0,
                                   lw=1, 
                                   color='k')
    for ax in axes.flatten():
        ax.set_xlabel('')
    
    plt.suptitle('Normalized Price Series')
    plt.gcf().tight_layout()
    sns.despine();
    
    """## Correlation"""
    
    sns.clustermap(df.corr(),
                   annot=True,
                   fmt='.2f',
                   cmap=sns.diverging_palette(h_neg=20,
                                              h_pos=220), center=0);
    
    """## Normalize Data
    
    We scale each series to the range [0, 1] using scikit-learn’s `MinMaxScaler` that we later use to rescale the synthetic data.
    """
    
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(df).astype(np.float32)
    
    """## Create rolling window sequences
    
    In the next step, we create rolling windows containing overlapping sequences of 24 consecutive data points for the six series:
    """
    
    data = []
    for i in range(len(df) - seq_len):
        data.append(scaled_data[i:i + seq_len])
    
    n_windows = len(data)
    
    """## Create tf.data.Dataset"""
    
    real_series = (tf.data.Dataset
                   .from_tensor_slices(data)
                   .shuffle(buffer_size=n_windows)
                   .batch(batch_size))
    real_series_iter = iter(real_series.repeat())
    
    """## Set up random series generator
    
    We also need a random time series generator that produces simulated data with 24 observations on the six series as long as the training continues.
    
    To this end, we create a generator that draws the requisite data uniform at random and feed the result into a second `tf.data.Dataset`. We set this `Dataset` to produce batches of the desired size and to repeat the process as long as necessary:
    """
    def make_Wiener_process(T,n_dim=1,mu=0,sigma=1,delta=0.1,if_print=False):
        #T=10
        #delta=0.01
        #mu, sigma, n_dim = 0.002, 0.1, 2 # 均值和标准差 
        N=int(T/delta)
        s = np.random.normal(0, delta, (N,n_dim))  
        time_trend=np.array([mu*np.arange(1,N+1)*delta for j in range(n_dim)]).T
        z=np.cumsum(s,1)*sigma+time_trend 
        if if_print==True:
            for i in range(n_dim):
                t=np.array(list(range(N)))*delta
                plt.plot(t,z,lw=0.2)
        return z
    
    
    #make_Wiener_process(T=240,n_dim=2,delta=0.1,if_print=True)
    
    # def make_random_data():
    #     while True:
    #         yield make_Wiener_process(seq_len,hidden_dim,delta=1)
    
    # def make_random_data():
    #     while True:
    #         yield np.random.uniform(low=0, high=1, size=(seq_len, hidden_dim))
    
    def make_random_data():
        while True:
            yield np.random.normal(loc=0.0, scale=1.0, size=(seq_len, n_seq))
    
    """We use the Python generator to feed a `tf.data.Dataset` that continues to call the random number generator as long as necessary and produces the desired batch size."""
    hidden_dim = int(n_seq*h)
    
    random_series = iter(tf.data.Dataset
                         .from_generator(make_random_data, output_types=tf.float32)
                         .batch(batch_size)
                         .repeat())
    
    """# TimeGAN Setup
    
    We’ll now create the two autoencoder components and the two adversarial network elements, as well as the supervisor that encourages the generator to learn the temporal dynamic of the historical price series.
    
    ##  Network Parameters
    
    We follow the authors’ sample code in creating RNNs with three hidden layers with 24 GRU units each, except for the supervisor that only uses two hidden layers.
    """
    
    
    num_layers = 3
    
    """## Set up logger"""
    
    writer = tf.summary.create_file_writer(log_dir.as_posix())
    
    """## Input place holders"""
    
    X = Input(shape=[seq_len, n_seq], name='RealData')
    Z = Input(shape=[seq_len, n_seq], name='RandomData')
    
    """## RNN block generator
    
    We keep it simple and use a very similar architecture for all four components. For a real-world application, they should be tailored to the data.
    
    The following `make_rnn` function automates the network creation:
    """
    
    # generator 1
    def make_rnn(n_layers, hidden_units, output_units, name):
        return Sequential([GRU(units=hidden_units,
                               return_sequences=True,
                               name=f'GRU_{i + 1}') for i in range(n_layers)] +
                          [Dense(units=output_units,
                                 activation='sigmoid',
                                 name='OUT')], name=name)
    
    # generator 2
    def make_lstm(n_layers, hidden_units, output_units, name):
        return Sequential([LSTM(units=hidden_units,
                               return_sequences=True,
                               name=f'LSTM_{i + 1}') for i in range(n_layers)] +
                          [Dense(units=output_units,
                                 activation='sigmoid',
                                 name='OUT')], name=name)
    
    # disc
    def make_brnn(n_layers, hidden_units, output_units, name):   # eg: input_shape=(5, 10)
        return Sequential([Bidirectional(LSTM(hidden_units, return_sequences=True),
                               #input_shape=input_shape,
                               name=f'BRNN_{i + 1}') for i in range(n_layers)] +
                          [Dense(units=output_units,
                                 activation='sigmoid',
                                 name='OUT')], name=name)
    
    
        # model = Sequential()
        # model.add(Bidirectional(LSTM(hidden_units, return_sequences=True),
        #                         input_shape=input_shape))
        # #model.add(Bidirectional(LSTM(10)))
        # model.add(Dense(output_units))
        # model.add(Activation('relu'))
        # #model.compile(loss='categorical_crossentropy', optimizer='rmsprop')    
        # return model
    
    # recover
    def make_fnn(n_layers, hidden_units, output_units, name):
        return Sequential([Dense(units=hidden_units,
                               name=f'FNN_{i + 1}') for i in range(n_layers)] +
                          [Dense(units=output_units,
                                 activation='sigmoid',
                                 name='OUT')], name=name)
    
    # encoder 
    def make_tcn(n_layers, kernel_size, output_units, name):
        return Sequential([Conv1D(filters=output_units, kernel_size=kernel_size, strides=1, padding='same',
                                  data_format='channels_last', dilation_rate=1, activation='sigmoid',
                                  use_bias=True, kernel_initializer='glorot_uniform',
                                  bias_initializer='zeros', kernel_regularizer=None,
                                  bias_regularizer=None, activity_regularizer=None,
                                  kernel_constraint=None,
                                  bias_constraint=None) for i in range(n_layers)], name=name)
    
    #embedder = make_tcn(n_layers=3, hidden_units=hidden_dim, output_units=hidden_dim,name='Embedder')
    
    """## Embedder & Recovery
    
    The autoencoder consists of the embedder and the recovery networks that we instantiate here:
    """
    
    # embedder = make_tcn(n_layers=3,
    #                     kernel_size=10,
    #                     output_units=hidden_dim, 
    #                     name='Embedder')
    
    embedder1 = make_tcn(n_layers=3,
                        kernel_size=10,
                        output_units=hidden_dim, 
                        name='Embedder_tcn')

    embedder2 = make_rnn(n_layers=3, 
                         hidden_units=hidden_dim, 
                         output_units=hidden_dim, 
                         name='Embedder_rnn')

    embedder3 = make_lstm(n_layers=3, 
                         hidden_units=hidden_dim, 
                         output_units=hidden_dim, 
                         name='Embedder_lstm') 

    embedder = eval('embedder'+str(e))    
    
    recovery = make_brnn(n_layers=1,
                        hidden_units=hidden_dim,
                        output_units=n_seq,
                        name='Recovery')
    
    """## Generator & Discriminator
    
    We then create the generator, the discriminator, and the supervisor like so:
    """
    
    # generator = make_rnn(n_layers=3, 
    #                      hidden_units=hidden_dim, 
    #                      output_units=hidden_dim, 
    #                      name='Generator')
    
    generator1 = make_tcn(n_layers=3,
                        kernel_size=10,
                        output_units=hidden_dim, 
                        name='generator_tcn')

    generator2 = make_rnn(n_layers=3, 
                         hidden_units=hidden_dim, 
                         output_units=hidden_dim, 
                         name='generator_rnn')

    generator3 = make_lstm(n_layers=3, 
                         hidden_units=hidden_dim, 
                         output_units=hidden_dim, 
                         name='generator_lstm') 

    generator = eval('generator'+str(g))    
    
    
    discriminator = make_brnn(n_layers=1,
                             hidden_units=hidden_dim, 
                             output_units=1, 
                             name='Discriminator')
    supervisor = make_rnn(n_layers=2, 
                          hidden_units=hidden_dim, 
                          output_units=hidden_dim, 
                          name='Supervisor')
    
    """# TimeGAN Training
    
    ## Settings
    """
    
    train_steps = 10
    gamma = 1
    
    """## Generic Loss Functions
    
    We also define two generic loss functions, namely `MeanSquaredError` and `BinaryCrossEntropy` that we will use later to create the various specific loss functions during the three phases:
    """
    
    mse = MeanSquaredError()
    bce = BinaryCrossentropy()
    
    """# Phase 1: Autoencoder Training
    
    ## Architecture
    
    The autoencoder integrates the embedder and the recovery functions:
    """
    
    H = embedder(X)
    X_tilde = recovery(H)
    
    autoencoder = Model(inputs=X,
                        outputs=X_tilde,
                        name='Autoencoder')
    
    autoencoder.summary()
    
    # plot_model(autoencoder,
    #            to_file=(results_path / 'autoencoder.png').as_posix(),
    #            show_shapes=True)
    
    """We next instantiate the optimizer for this training phase and define the training step. It follows the pattern introduced with the DCGAN example, using tf.GradientTape to record the operations that generate the reconstruction loss. This allows us to rely on the automatic differentiation engine to obtain the gradients with respect to the trainable embedder and recovery network weights that drive backpropagation:
    
    ## Autoencoder Optimizer
    """
    
    autoencoder_optimizer = Adam()
    
    """## Autoencoder Training Step"""
    
    @tf.function
    def train_autoencoder_init(x):
        with tf.GradientTape() as tape:
            x_tilde = autoencoder(x)
            embedding_loss_t0 = mse(x, x_tilde)
            e_loss_0 = 10 * tf.sqrt(embedding_loss_t0)
    
        var_list = embedder.trainable_variables + recovery.trainable_variables
        gradients = tape.gradient(e_loss_0, var_list)
        autoencoder_optimizer.apply_gradients(zip(gradients, var_list))
        return tf.sqrt(embedding_loss_t0)
    
    """## Autoencoder Training Loop
    
    The reconstruction loss simply compares the autoencoder outputs with its inputs. We train for 10,000 steps in a little over one minute using this training loop that records the step loss for monitoring with TensorBoard:
    """
    
    for step in tqdm(range(train_steps)):
        X_ = next(real_series_iter)
        step_e_loss_t0 = train_autoencoder_init(X_)
        with writer.as_default():
            tf.summary.scalar('Loss Autoencoder Init', step_e_loss_t0, step=step)
    
    """## Persist model"""
    
    # autoencoder.save(log_dir / 'autoencoder')
    
    """# Phase 2: Supervised training
    
    We already created the supervisor model so we just need to instantiate the optimizer and define the train step as follows:
    
    ## Define Optimizer
    """
    
    supervisor_optimizer = Adam()
    
    """## Train Step"""
    
    @tf.function
    def train_supervisor(x):   #x=X_
        with tf.GradientTape() as tape:
            h = embedder(x)
            h_hat_supervised = supervisor(h)
            g_loss_s = mse(h[:, 1:, :], h_hat_supervised[:, :-1, :])
    
        var_list = supervisor.trainable_variables
        gradients = tape.gradient(g_loss_s, var_list)
        supervisor_optimizer.apply_gradients(zip(gradients, var_list))
        return g_loss_s
    
    """In this case, the loss compares the output of the supervisor with the next timestep for the embedded sequence so that it learns the temporal dynamics of the historical price sequences; the training loop works similarly to the autoencoder example.
    
    ## Training Loop
    """
    
    for step in tqdm(range(train_steps)):
        X_ = next(real_series_iter)
        step_g_loss_s = train_supervisor(X_)
        with writer.as_default():
            tf.summary.scalar('Loss Generator Supervised Init', step_g_loss_s, step=step)
        if int(step)%1000==0:
            print('Loss Generator Supervised Init', int(step),step_g_loss_s)
    
    
    #supervisor.summary()
    """## Persist Model"""
    
    # supervisor.save(log_dir / 'supervisor')
    
    """# Joint Training
    
    The joint training involves all four network components as well as the supervisor. It uses multiple loss functions and combinations of the base components to achieve the simultaneous learning of latent space embeddings, transition dynamics, and synthetic data generation.
    
    ## Generator
    
    ### Adversarial Architecture - Supervised
    """
    
    E_hat = generator(Z)
    H_hat = supervisor(E_hat)
    Y_fake = discriminator(H_hat)
    
    adversarial_supervised = Model(inputs=Z,
                                   outputs=Y_fake,
                                   name='AdversarialNetSupervised')
    
    adversarial_supervised.summary()
    
    #plot_model(adversarial_supervised, show_shapes=True)
    
    """### Adversarial Architecture in Latent Space"""
    
    Y_fake_e = discriminator(E_hat)
    
    adversarial_emb = Model(inputs=Z,
                        outputs=Y_fake_e,
                        name='AdversarialNet')
    
    adversarial_emb.summary()
    
    #plot_model(adversarial_emb, show_shapes=True)
    
    """### Mean & Variance Loss
    
    To ensure that the generator faithfully reproduces the time series, TimeGAN includes a moment loss that penalizes when mean and variance of the synthetic data deviate from the real version:
    """
    
    X_hat = recovery(H_hat)
    synthetic_data = Model(inputs=Z,
                           outputs=X_hat,
                           name='SyntheticData')
    
    """The end-to-end model that produces synthetic data involves the generator, supervisor, and recovery component. It is defined as follows and has close to 30,000 trainable paramters:"""
    
    synthetic_data.summary()
    
    #plot_model(synthetic_data, show_shapes=True)
    
    def get_generator_moment_loss(y_true, y_pred):
        y_true_mean, y_true_var = tf.nn.moments(x=y_true, axes=[0])
        y_pred_mean, y_pred_var = tf.nn.moments(x=y_pred, axes=[0])
        g_loss_mean = tf.reduce_mean(tf.abs(y_true_mean - y_pred_mean))
        g_loss_var = tf.reduce_mean(tf.abs(tf.sqrt(y_true_var + 1e-6) - tf.sqrt(y_pred_var + 1e-6)))
        return g_loss_mean + g_loss_var
    
    
    
    # def get_generator_conditional_loss(x_encoded):   #x_encoded=temp1
        
    #     out=[]
    #     for i in range():
    #         out.append(generator.layer[0].reset_state(states=x_encoded[i]))
    #     out=np.array(out)
        
    #     return tf.reduce_mean(tf.abs(out-x_encoded[1:]))
    """## Discriminator
    
    ### Architecture: Real Data
    """
    
    Y_real = discriminator(H)
    discriminator_model = Model(inputs=X,
                                outputs=Y_real,
                                name='DiscriminatorReal')
    
    discriminator_model.summary()
    
    #plot_model(discriminator_model, show_shapes=True)
    
    """## Optimizers
    
    The joint training involves three optimizers for the autoencoder, the generator and the discriminator:
    """
    
    generator_optimizer = Adam()
    discriminator_optimizer = Adam()
    embedding_optimizer = Adam()
    
    """## Generator Train Step
    
    The train step for the generator illustrates the use of four loss functions and corresponding combinations of network components to achieve the desired learning outlined at the beginning of this section:
    """
    #generator.summary()
    
    
    @tf.function
    def train_generator(x, z):     #x=X_  z=Z_
        with tf.GradientTape() as tape:
            y_fake = adversarial_supervised(z)
            generator_loss_unsupervised = bce(y_true=tf.ones_like(y_fake),
                                              y_pred=y_fake)
    
            y_fake_e = adversarial_emb(z)
            generator_loss_unsupervised_e = bce(y_true=tf.ones_like(y_fake_e),
                                                y_pred=y_fake_e)
            h = embedder(x)
            h_hat_supervised = supervisor(h)
            generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, :-1, :])
    
            x_hat = synthetic_data(z)
            generator_moment_loss = get_generator_moment_loss(x, x_hat)
            
            # x_con = get_generator_conditional_loss(h)
            # generator_conditional_loss = bce(h[:,1:,:],x_con)
    
            generator_loss = (generator_loss_unsupervised +
                              generator_loss_unsupervised_e +
                              100 * tf.sqrt(generator_loss_supervised) +
                              100 * generator_moment_loss)# +
                              #generator_conditional_loss)
    
        var_list = generator.trainable_variables + supervisor.trainable_variables
        gradients = tape.gradient(generator_loss, var_list)
        generator_optimizer.apply_gradients(zip(gradients, var_list))
        return generator_loss_unsupervised, generator_loss_supervised, generator_moment_loss
    
    """## Embedding Train Step"""
    
    @tf.function
    def train_embedder(x):
        with tf.GradientTape() as tape:
            h = embedder(x)
            h_hat_supervised = supervisor(h)
            generator_loss_supervised = mse(h[:, 1:, :], h_hat_supervised[:, :-1, :])
    
            x_tilde = autoencoder(x)
            embedding_loss_t0 = mse(x, x_tilde)
            e_loss = 10 * tf.sqrt(embedding_loss_t0) + 0.1 * generator_loss_supervised
    
        var_list = embedder.trainable_variables + recovery.trainable_variables
        gradients = tape.gradient(e_loss, var_list)
        embedding_optimizer.apply_gradients(zip(gradients, var_list))
        return tf.sqrt(embedding_loss_t0)
    
    """## Discriminator Train Step"""
    
    @tf.function
    def get_discriminator_loss(x, z):
        y_real = discriminator_model(x)
        discriminator_loss_real = bce(y_true=tf.ones_like(y_real),
                                      y_pred=y_real)
    
        y_fake = adversarial_supervised(z)
        discriminator_loss_fake = bce(y_true=tf.zeros_like(y_fake),
                                      y_pred=y_fake)
    
        y_fake_e = adversarial_emb(z)
        discriminator_loss_fake_e = bce(y_true=tf.zeros_like(y_fake_e),
                                        y_pred=y_fake_e)
        return (discriminator_loss_real +
                discriminator_loss_fake +
                gamma * discriminator_loss_fake_e)
    
    @tf.function
    def train_discriminator(x, z):
        with tf.GradientTape() as tape:
            discriminator_loss = get_discriminator_loss(x, z)
    
        var_list = discriminator.trainable_variables
        gradients = tape.gradient(discriminator_loss, var_list)
        discriminator_optimizer.apply_gradients(zip(gradients, var_list))
        return discriminator_loss
    
    """## Training Loop
    
    Finally, the joint training loop pulls the various training steps together and builds on the learning from phase 1 and 2 to train the TimeGAN components on both real and random data. We run the loop for 10,000 iterations in under 50 minutes:
    """
    import scipy
    import pandas as pd
    from statsmodels.tsa.stattools import acf
    
    x2_returns=[]
    for i in range(len(df)-seq_len):
        x2_returns.append(np.log(df.iloc[i+seq_len,:]/df.iloc[i,:]))
    x2_returns=np.array(x2_returns)
    x2_returns=pd.DataFrame(x2_returns)
    
    
    
    density_metric=[]
    kurtosis_metric=[]
    skewness_metric=[]
    
    def save_reports():
    
        generated_data = []
        for i in range(5):
            Z_ = next(random_series)
            d = synthetic_data(Z_)
            generated_data.append(d)    
        generated_data = np.array(np.vstack(generated_data))
        
        #==========density
        x1_returns=[]
        for i in range(len(generated_data)):
            x1_returns.append(np.log(generated_data[i][-1,:]/generated_data[i][0,:]))
        x1_returns=np.array(x1_returns)
        
        
        discrepancy=[]
        for i in range(n_seq):
            t=scipy.stats.wasserstein_distance(x1_returns[:,i], x2_returns.iloc[:,i])
            if t>-1e5:
                discrepancy.append(t)
        density_metric.append(np.mean(discrepancy))
        #==========kurtosis
        x1_kurt=[]
        for i in range(len(generated_data)):
            x1_kurt.append(pd.DataFrame(generated_data[i]).pct_change().kurtosis())
        
        
        kurtosis_metric.append(0.1*np.linalg.norm(np.array(x1_kurt).mean(0)-df.pct_change().kurtosis(),ord=2))
        
        #==========skewness
        x1_skew=[]
        for i in range(len(generated_data)):
            x1_skew.append(pd.DataFrame(generated_data[i]).pct_change().skew())
        
        
        skewness_metric.append(np.linalg.norm(np.array(x1_skew).mean(0)-df.pct_change().skew(),ord=2))
        
    
        
        
        
    
    step_g_loss_u = step_g_loss_s = step_g_loss_v = step_e_loss_t0 = step_d_loss = 0
    for step in range(train_steps):
        # Train generator (twice as often as discriminator)
        for kk in range(2):
            X_ = next(real_series_iter)
            Z_ = next(random_series)
    
            # Train generator
            step_g_loss_u, step_g_loss_s, step_g_loss_v = train_generator(X_, Z_)
            # Train embedder
            step_e_loss_t0 = train_embedder(X_)
    
        X_ = next(real_series_iter)
        Z_ = next(random_series)
        step_d_loss = get_discriminator_loss(X_, Z_)
        if step_d_loss > 0.15:
            step_d_loss = train_discriminator(X_, Z_)
    
        if step % 1000 == 0:
            print(f'{step:6,.0f} | d_loss: {step_d_loss:6.4f} | g_loss_u: {step_g_loss_u:6.4f} | '
                  f'g_loss_s: {step_g_loss_s:6.4f} | g_loss_v: {step_g_loss_v:6.4f} | e_loss_t0: {step_e_loss_t0:6.4f}')
    
        with writer.as_default():
            tf.summary.scalar('G Loss S', step_g_loss_s, step=step)
            tf.summary.scalar('G Loss U', step_g_loss_u, step=step)
            tf.summary.scalar('G Loss V', step_g_loss_v, step=step)
            tf.summary.scalar('E Loss T0', step_e_loss_t0, step=step)
            tf.summary.scalar('D Loss', step_d_loss, step=step)
        
        if step%100==0:
            save_reports()
    """## Persist Synthetic Data Generator"""
    
    synthetic_data.save(log_dir / 'synthetic_data')
    
    """# Generate Synthetic Data
    
    To evaluate the TimeGAN results, we generate synthetic by drawing random inputs and feeding them to the `synthetic_data` network just described. 
    
    More specifically, we’ll create roughly as many artificial series with 24 observations on the six tickers as there are overlapping windows in the real dataset:
    """
    
    generated_data = []
    for i in range(int(n_windows / batch_size)):
        Z_ = next(random_series)
        d = synthetic_data(Z_)
        generated_data.append(d)
    
    len(generated_data)
    
    """The result is 35 batches containing 128 samples with dimensions 24⨉6 each that we stack like so:"""
    
    generated_data = np.array(np.vstack(generated_data))
    generated_data.shape
    
    np.save(path+'generated_data_raw.npy', generated_data) #/content/drive/MyDrive/TimeGAN0101Detail/
    
    
    """## Rescale
    
    We can use the trained MinMaxScaler to revert the synthetic to the scale of the input series:
    """
    
    generated_data = (scaler.inverse_transform(generated_data
                                               .reshape(-1, n_seq))
                      .reshape(-1, seq_len, n_seq))
    generated_data.shape
    
    
    """## Persist Data"""
    np.save(path+'generated_data.npy', generated_data) #/content/drive/MyDrive/TimeGAN0101Detail/
    
    # with pd.HDFStore(hdf_store) as store:
    #     store.put('data/synthetic', pd.DataFrame(generated_data.reshape(-1, n_seq),
    #                                              columns=tickers))
    '''
    import pandas as pd
    with pd.HDFStore(hdf_store) as store:
        temp=store.get('data/synthetic')
    '''
    
    
    """## Plot sample Series
    
    The below figure displays samples of the six synthetic series and the corresponding real series. The synthetic data generally reflect a variation of behavior not unlike their real counterparts and, after rescaling, roughly (due to the random input) match their range.
    """
    
    fig, axes = plt.subplots(nrows=8, ncols=2, figsize=(14, 20))
    axes = axes.flatten()
    
    index = list(range(1, seq_len+1))
    synthetic = generated_data[np.random.randint(100)]
    
    idx = np.random.randint(len(df) - seq_len)
    real = df.iloc[idx: idx + seq_len]
    
    for j, ticker in enumerate(tickers):
        (pd.DataFrame({'Real': real.iloc[:, j].values,
                       'Synthetic': synthetic[:, j]})
         .plot(ax=axes[j],
               title=ticker,
               secondary_y='Synthetic', style=['-', '--'],
               lw=1))
    sns.despine()
    fig.tight_layout()
    plt.savefig(path+'/1.jpg')
    
    
    
    #================================================220101
    def make_random_data_long():
        while True:
            yield np.random.normal(loc=0.0, scale=1.0, size=(seq_len*100, n_seq))
    
    
    random_series = iter(tf.data.Dataset
                         .from_generator(make_random_data_long, output_types=tf.float32)
                         .batch(batch_size)
                         .repeat())
    
    
    
    generated_data_l = []
    for i in range(1):
        Z_ = next(random_series)
        d = synthetic_data(Z_)
        generated_data_l.append(d)
    
    len(generated_data_l)
    
    """The result is 35 batches containing 128 samples with dimensions 24⨉6 each that we stack like so:"""
    
    generated_data_l = np.array(np.vstack(generated_data_l))
    
    
    generated_data_l = (scaler.inverse_transform(generated_data_l.reshape(-1, n_seq)).reshape(-1, seq_len*100, n_seq))
    generated_data_l.shape
    
    """## Persist Data"""
    np.save(path+'generated_data_long.npy', generated_data_l)  #/content/drive/MyDrive/TimeGAN0101Detail/
    
    
    
    import matplotlib.dates as mdates
    
    index = df.index[:generated_data_l.shape[1]]
    
    for i in range(n_seq):
        fig, ax = plt.subplots(1,1, figsize=(12, 6))
        ax.plot(index, generated_data_l[0,:,i], color='C1', alpha=0.2, label='synthetic')
        for p in generated_data_l[1:,:,i]:
            ax.plot(index, p, color='C1', alpha=0.2)
            
        ax.plot(index, df.iloc[:generated_data_l.shape[1],i].values, label='historical')
        ax.set_ylabel('Spot price')
        ax.set_xlabel('Year')
        ax.grid()
        leg = ax.legend()
        datemin = np.datetime64(index[0], 'Y')
        datemax = np.datetime64(index[-1], 'Y') + np.timedelta64(1, 'Y')
        ax.set_xlim(datemin, datemax)
        for l in leg.get_lines():
            l.set_alpha(1)
        
        years = mdates.YearLocator()   # every year
        months = mdates.MonthLocator()  # every month
        years_fmt = mdates.DateFormatter('%Y')
        
        ax.xaxis.set_major_locator(years)
        ax.xaxis.set_major_formatter(years_fmt)
        ax.xaxis.set_minor_locator(months)
        
        ax.spines['right'].set_visible(False)
        ax.spines['top'].set_visible(False)
        ax.spines['bottom'].set_visible(False)
        plt.savefig(path+'/7_'+str(i)+'.jpg')
    
    #=================================================211224
    from pathlib import Path
    
    import numpy as np
    import pandas as pd
    
    from sklearn.manifold import TSNE
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import MinMaxScaler
    
    import tensorflow as tf
    from tensorflow.keras.losses import BinaryCrossentropy, MeanSquaredError, MeanAbsoluteError
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.metrics import AUC
    
    import matplotlib.pyplot as plt
    from matplotlib.ticker import FuncFormatter
    import seaborn as sns
    
    gpu_devices = tf.config.experimental.list_physical_devices('GPU')
    if gpu_devices:
        print('Using GPU')
        tf.config.experimental.set_memory_growth(gpu_devices[0], True)
    else:
        print('Using CPU')
    
    sns.set_style('white')
    
    np.random.seed(42)
    tf.random.set_seed(1234)
    
    path = Path('time_gan')
    hdf_store = path / 'TimeSeriesGAN.h5'
    
    seq_len = 24
    n_seq = 16
    
    """# Load Data"""
    
    experiment = 0
    
    def get_real_data():
        df = pd.read_csv(dloc+'stockss.csv',  #/content/drive/MyDrive/TimeGAN/
                         index_col='Date', 
                         parse_dates=['Date']).sort_index()
    
        # Preprocess the dataset:
        scaler = MinMaxScaler()
        scaled_data = scaler.fit_transform(df)
    
        data = []
        for i in range(len(df) - seq_len):
            data.append(scaled_data[i:i + seq_len])
        return data
    
    
    real_data = get_real_data()
    
    n = len(real_data)
    
    np.asarray(real_data).shape
    
    synthetic_data = np.load(path+'generated_data_raw.npy') #/content/drive/MyDrive/TimeGAN0101Detail/
    synthetic_data.shape
    synthetic_data=synthetic_data[:8550]
    
    real_data = real_data[:synthetic_data.shape[0]]
    
    """# Prepare Sample"""
    
    sample_size = 250
    idx = np.random.permutation(len(real_data))[:sample_size]
    
    # Data preprocessing
    real_sample = np.asarray(real_data)[idx]
    synthetic_sample = np.asarray(synthetic_data)[idx]
    
    real_sample_2d = real_sample.reshape(-1, seq_len)
    synthetic_sample_2d = synthetic_sample.reshape(-1, seq_len)
    
    real_sample_2d.shape, synthetic_sample_2d.shape
    
    
    """# Visualization in 2D: A Qualitative Assessment of Diversity
    
    To visualize the real and synthetic series with 24 time steps and six features, we reduce their dimensionality so that we can plot them in two dimensions. To this end, we sample 250 normalized sequences with six features each and reshape them to obtain data with dimensionality 1,500 ⨉ 24.
    
    ## Run PCA
    
    PCA is a linear method that identifies a new basis with mutually orthogonal vectors that, successively, capture the directions of maximum variance in the data. We compute the first two components using the real data and then project both real and synthetic samples onto the new coordinate system:
    """
    
    pca = PCA(n_components=2)
    pca.fit(real_sample_2d)
    pca_real = (pd.DataFrame(pca.transform(real_sample_2d))
                .assign(Data='Real'))
    pca_synthetic = (pd.DataFrame(pca.transform(synthetic_sample_2d))
                     .assign(Data='Synthetic'))
    pca_result = pca_real.append(pca_synthetic).rename(
        columns={0: '1st Component', 1: '2nd Component'})
    
    """## Run t-SNE
    
    t-SNE is a non-linear manifold learning method for the visualization of high-dimensional data. It converts similarities between data points to joint probabilities and aims to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data (see Chapter 13). We compute t-SNE for the combined real and synthetic data as follows:
    """
    
    tsne_data = np.concatenate((real_sample_2d,
                                synthetic_sample_2d), axis=0)
    
    tsne = TSNE(n_components=2,
                verbose=1,
                perplexity=40)
    tsne_result = tsne.fit_transform(tsne_data)
    
    tsne_result = pd.DataFrame(tsne_result, columns=['X', 'Y']).assign(Data='Real')
    tsne_result.loc[sample_size*6:, 'Data'] = 'Synthetic'
    
    """## Plot Result
    
    The below figure displays the PCA and t-SNE results for a qualitative assessment of the similarity of the real and synthetic data distributions. Both methods reveal strikingly similar patterns and significant overlap, suggesting that the synthetic data capture important aspects of the real data characteristics.
    """
    
    fig, axes = plt.subplots(ncols=2, figsize=(14, 5))
    
    sns.scatterplot(x='1st Component', y='2nd Component', data=pca_result,
                    hue='Data', style='Data', ax=axes[0])
    sns.despine()
    axes[0].set_title('PCA Result')
    
    
    sns.scatterplot(x='X', y='Y',
                    data=tsne_result,
                    hue='Data', 
                    style='Data', 
                    ax=axes[1])
    sns.despine()
    for i in [0, 1]:
        axes[i].set_xticks([])
        axes[i].set_yticks([])
    
    axes[1].set_title('t-SNE Result')
    fig.suptitle('Assessing Diversity: Qualitative Comparison of Real and Synthetic Data Distributions', 
                 fontsize=14)
    fig.tight_layout()
    fig.subplots_adjust(top=.88);
    plt.savefig(path+'/2.jpg')
    
    """# Time Series Classification: A quantitative Assessment of Fidelity
    
    The visualization only provides a qualitative impression. For a quantitative assessment of the fidelity of the synthetic data, we train a time-series classifier to distinguish between real and fake data and evaluate its performance on a held-out test set.
    
    More specifically, we select the first 80 percent of the rolling sequences for training, and the last 20 percent as test set as follows:
    
    ## Prepare Data
    """
    
    real_data = get_real_data()
    real_data = np.array(real_data)[:len(synthetic_data)]
    real_data.shape
    
    synthetic_data.shape
    
    n_series = real_data.shape[0]
    
    idx = np.arange(n_series)
    
    n_train = int(.8*n_series)
    train_idx = idx[:n_train]
    test_idx = idx[n_train:]
    
    train_data = np.vstack((real_data[train_idx], 
                            synthetic_data[train_idx]))
    test_data = np.vstack((real_data[test_idx], 
                           synthetic_data[test_idx]))
    
    n_train, n_test = len(train_idx), len(test_idx)
    train_labels = np.concatenate((np.ones(n_train),
                                   np.zeros(n_train)))
    test_labels = np.concatenate((np.ones(n_test),
                                  np.zeros(n_test)))
    
    """## Create Classifier
    
    Then we create a simple RNN with six units that receives mini batches of real and synthetic series of shape 24 ⨉ 6 and uses a sigmoid activation. We optimize it using binary cross entropy loss and the Adam optimizer while tracking the AUC and accuracy metrics:
    """
    
    ts_classifier = Sequential([GRU(6, input_shape=(24, 16), name='GRU'),
                                Dense(1, activation='sigmoid', name='OUT')],
                               name='Time_Series_Classifier')
    
    ts_classifier.compile(loss='binary_crossentropy',
                          optimizer='adam',
                          metrics=[AUC(name='AUC'), 'accuracy'])
    
    """The model has 259 trainable parameters. We train it for 250 epochs on batches of 128 randomly selected samples and track the validation performance:"""
    
    ts_classifier.summary()
    
    result = ts_classifier.fit(x=train_data,
                               y=train_labels,
                               validation_data=(test_data, test_labels),
                               epochs=250,
                               batch_size=128,
                               verbose=0)
    
    """Once the training completes, evaluation of the test set yields a classification error of almost 56 percent on the balanced test set, and a very low AUC of 0.136."""
    
    ts_classifier.evaluate(x=test_data, y=test_labels)
    
    history = pd.DataFrame(result.history)
    history.info()
    
    """The plot shows that that model is not able to learn the difference between the real and synthetic data in a way that generalizes to the test set. This result suggests that the quality of the synthetic data meets the fidelity standard."""
    
    sns.set_style('white')
    fig, axes = plt.subplots(ncols=2, figsize=(14,4))
    history[['AUC', 'val_AUC']].rename(columns={'AUC': 'Train', 'val_AUC': 'Test'}).plot(ax=axes[1], 
                                                                                         title='ROC Area under the Curve',
                                                                                        style=['-', '--'],
                                                                                        xlim=(0, 250))
    history[['accuracy', 'val_accuracy']].rename(columns={'accuracy': 'Train', 'val_accuracy': 'Test'}).plot(ax=axes[0], 
                                                                                                             title='Accuracy',
                                                                                                            style=['-', '--'],
                                                                                                            xlim=(0, 250))
    for i in [0, 1]:
        axes[i].set_xlabel('Epoch')
    
    axes[0].yaxis.set_major_formatter(FuncFormatter(lambda y, _: '{:.0%}'.format(y))) 
    axes[0].set_ylabel('Accuracy (%)')
    axes[1].set_ylabel('AUC')
    sns.despine()
    fig.suptitle('Assessing Fidelity: Time Series Classification Performance', fontsize=14)
    fig.tight_layout()
    fig.subplots_adjust(top=.85);
    plt.savefig(path+'/3.jpg')
    """# Train on synthetic, test on real: Assessing usefulness
    
    Finally, we want to know how useful synthetic data is when it comes to solving a prediction problem. To this end, we train a time-series prediction model alternatively on the synthetic and the real data to predict the next time step and compare the performance on a test set create from the real data.
    
    ## Prepare Data
    """
    
    real_data = get_real_data()
    real_data = np.array(real_data)[:len(synthetic_data)]
    
    real_data.shape, synthetic_data.shape
    
    """More specifically, we select the first 23 time steps of each sequence as input, and the final time step as output. At the same time, we split the real data into train and test set using the same temporal split as in the previous classification example: """
    
    real_train_data = real_data[train_idx, :seq_len-1, :]
    real_train_label = real_data[train_idx, -1, :]
    
    real_test_data = real_data[test_idx, :seq_len-1, :]
    real_test_label = real_data[test_idx, -1, :]
    
    real_train_data.shape, real_train_label.shape, real_test_data.shape, real_test_label.shape
    
    """We select the complete synthetic data for training since abundance is one of the reasons we generated it in the first place:"""
    
    synthetic_train = synthetic_data[:, :seq_len-1, :]
    synthetic_label = synthetic_data[:, -1, :]
    
    synthetic_train.shape, synthetic_label.shape
    
    """## Create RNN
    
    We create a one-layer RNN with 12 GRU units that predicts the last time steps for the six stock price series and, thus, has six linear output units. The model uses the Adam optimizer to minimize the mean absolute error (MAE):
    """
    
    def get_model():
        model = Sequential([GRU(12, 
                                input_shape=(seq_len-1, 
                                             n_seq)),
                            Dense(n_seq)])
    
        model.compile(optimizer=Adam(),
                      loss=MeanAbsoluteError(name='MAE'))
        return model
    
    """## Train model
    
    We train the model twice using the synthetic and real data for training, respectively, and the real test set to evaluate the out-of-sample performance. Training on synthetic data works as follows, training on real data works analogously (see notebook):
    """
    
    ts_regression = get_model()
    synthetic_result = ts_regression.fit(x=synthetic_train,
                                         y=synthetic_label,
                                         validation_data=(
                                             real_test_data, 
                                             real_test_label),
                                         epochs=100,
                                         batch_size=128,
                                         verbose=0)
    
    synthetic_result = pd.DataFrame(synthetic_result.history).rename(columns={'loss': 'Train', 
                                                                              'val_loss': 'Test'})
    
    ts_regression = get_model()
    real_result = ts_regression.fit(x=real_train_data,
                                    y=real_train_label,
                                    validation_data=(
                                        real_test_data, 
                                        real_test_label),
                                    epochs=100,
                                    batch_size=128,
                                    verbose=0)
    
    """## Evaluate results"""
    
    real_result = pd.DataFrame(real_result.history).rename(columns={'loss': 'Train', 
                                                                    'val_loss': 'Test'})
    
    """The below figure plots the MAE on the train and test set (on a log scale so we can spot the difference) for both models. It turns out that the MAE is slightly lower after training on the synthetic dataset."""
    
    fig, axes = plt.subplots(ncols=2, figsize=(14, 4), sharey=True)
    
    synthetic_result.plot(ax=axes[0], title='Train on Synthetic, Test on Real', 
                          logy=True, 
                          xlim=(0, 100))
    real_result.plot(ax=axes[1], title='Train on Real, Test on Real', 
                     logy=True, 
                     xlim=(0, 100))
    
    for i in [0, 1]:
        axes[i].set_xlabel('Epoch')
        axes[i].set_ylabel('Mean Absolute Error (log scale)')
    
    sns.despine()
    fig.suptitle('Assessing Usefulness: Time Series Prediction Performance', fontsize=14)
    fig.tight_layout()
    fig.subplots_adjust(top=.85);
    plt.savefig(path+'/4.jpg')
    
    
    #==============================================211225
    import matplotlib.pyplot as plt
    
    'log return distribution: real vs synthetic ==> quantify: find some distance between distribution'
    x1_returns=[]
    for i in range(len(generated_data)):
        x1_returns.append(np.log(generated_data[i][-1,:]/generated_data[i][0,:]))
    x1_returns=np.array(x1_returns)
    
    
    x2_returns=[]
    for i in range(len(real_data)):
        x2_returns.append(np.log(df.iloc[i+seq_len,:]/df.iloc[i,:]))
    x2_returns=np.array(x2_returns)
    
    for i in range(n_seq//2):
        plt.rcParams['figure.figsize'] = (13, 5)    #设定图片大小
        f = plt.figure()                            #确定画布
        
        f.add_subplot(1,2,1)
        sns.distplot(x1_returns[:,2*i], kde=False,label='synthetic')                 #绘制频数直方图
        sns.distplot(x2_returns[:,2*i], kde=False,label='true')                 #绘制频数直方图
        plt.ylabel("Probability Density", fontsize=16)
        plt.xticks(fontsize=16)                    #设置x轴刻度值的字体大小
        plt.yticks(fontsize=16)                   #设置y轴刻度值的字体大小
        plt.title("("+tickers[2*i]+")", fontsize=20)             #设置子图标题
        
        f.add_subplot(1,2,2)
        sns.distplot(x1_returns[:,2*i+1],kde=False,label='synthetic')                           #绘制密度直方图
        sns.distplot(x2_returns[:,2*i+1],kde=False,label='true')
        plt.xticks(fontsize=16)                  #设置x轴刻度值的字体大小
        plt.yticks(fontsize=16)                  #设置y轴刻度值的字体大小
        plt.title("("+tickers[2*i+1]+")", fontsize=20)            #设置子图标题
        
        plt.subplots_adjust(wspace=0.3)         #调整两幅子图的间距
        plt.legend()
        plt.show()
    #i=0
    
    discrepancy=[]
    for i in range(n_seq):
        t=scipy.stats.wasserstein_distance(x1_returns[:,i], x2_returns[:,i])
        if t > -1e5:
            discrepancy.append(t)
    
    #print('average wasserstein distance of log return',np.mean(discrepancy))
    f = open(path+"/discrepancy.txt",'a') 
    f.write(str([h,e,g])+'average wasserstein distance of log return: '+str(np.mean(discrepancy)))
    f.write('\n')
    f.close()    
    
    'moments, how is moment changing with train goes on. 2nd order, 3rd order, and 4th order ==> quantify: make reports'
    #plot test metrics. Note: We did not optimize for them in the training step.
    # We are just checking, if maybe we are lucky, and our generator is able to 
    # correctly compute those statistics as well. The only metric we considered in the training
    # is the density, because our loss directly depends on the distribution of the generated data
    # and the S&P500 returns. However, acf is a time-series based statistics. We indirectly
    # consider that because we take rolling averages. 
    ylabels = [ '$|\hat df_r - \hat df_G |_1$', '$|\hat \kappa_r - \hat \kappa_G |$', '$|\hat s_r - \hat s_G |$' ]
    titles = [ 'Density metric', 'Kurtosis metric', 'Skewness metric']
    
    fig, ax = plt.subplots(1, 3, figsize=(12, 4))
    ax = ax.ravel()
    training_loss=[density_metric,kurtosis_metric,skewness_metric]
    for i in range(3):
        ax[i].plot(training_loss[i], 'o', alpha=0.2)
        ax[i].set_ylabel(ylabels[i])
        ax[i].set_title(titles[i])
        ax[i].set_xlabel('# of training steps in hundreds')
        ax[i].set_ylim(bottom=0.)
        ax[i].grid()
    plt.tight_layout()
    plt.savefig(path+'/6.jpg')
    
    'conditional distribution'
    
    # 'ACF/leverage effect'
    # def compare_acf(x_real, x_fake, ax=None, max_lag=64, CI=True):
    #     """ Computes ACF of historical and (mean)-ACF of generated and plots those. """
    #     if ax is None:
    #         _, ax = plt.subplots(1, 1)
    #     acf_real_list = cacf_torch(x_real, max_lag=max_lag, dim=(0, 1)).cpu().numpy()
    #     acf_real = np.mean(acf_real_list, axis=0)
    
    #     acf_fake_list = cacf_torch(x_fake, max_lag=max_lag, dim=(0, 1)).cpu().numpy()
    #     acf_fake = np.mean(acf_fake_list, axis=0)
    
    #     ax.plot(acf_real[1:], label='Historical')
    #     ax.plot(acf_fake[1:], label='Generated', alpha=0.8)
    
    #     if CI:
    #         acf_fake_std = np.std(acf_fake_list, axis=0)
    #         ub = acf_fake + acf_fake_std
    #         lb = acf_fake - acf_fake_std
    
    #         for i in range(acf_real.shape[-1]):
    #             ax.fill_between(
    #                 range(acf_fake[1:, i].shape[0]),
    #                 ub[1:, i], lb[1:, i],
    #                 color='orange',
    #                 alpha=.3
    #             )
    #     set_style(ax)
    #     ax.set_xlabel('Lags')
    #     ax.set_ylabel('ACF')
    #     ax.grid(True)
    #     ax.legend()
    #     return ax
    
    # def compare_lev_eff(x_real, x_fake, ax=None):
    #     """ Computes ACF of historical and (mean)-ACF of generated and plots those. """
    #     if ax is None:
    #         _, ax = plt.subplots(1, 1)
    #     acf_real_list = acf(x_real, 32, dim=(1)).cpu().numpy()
    #     acf_real = np.mean(acf_real_list, axis=0)
    #     acf_fake_list = lev_eff_torch(x_fake, 32, dim=(1)).cpu().numpy()
    #     acf_fake = np.mean(acf_fake_list, axis=0)
    #     acf_fake_std = np.std(acf_fake_list, axis=0)
    #     ub = acf_fake + acf_fake_std
    #     lb = acf_fake - acf_fake_std
    
    #     ax.plot(acf_real[1:], label='Historical')
    #     ax.plot(acf_fake[1:], label='Generated', alpha=0.8)
    #     ax.fill_between(range(acf_fake[1:].shape[0]), ub[1:], lb[1:],
    #                     color='orange', alpha=.3)
    #     set_style(ax)
    #     ax.set_xlabel('Lags')
    #     ax.set_ylabel('Leverage Effect')
    #     ax.grid(True)
    #     ax.legend()
    #     return ax
    
    # def comparison(x_fake, x_real, rfs=10, figsize=(10, 8)):
    
    #     _, axes = plt.subplots(1, 3, figsize=figsize)
    
    #     compare_acf(x_real=x_real, x_fake=x_fake, ax=axes[0], max_lag=rfs)
    #     compare_acf(x_real=np.abs(x_real), x_fake=np.abs(x_fake), ax=axes[1], max_lag=rfs)
    #     compare_lev_eff(x_real=x_real, x_fake=x_fake, ax=axes[2])
    
    #     axes[0].title.set_text('Serial ACF')
    #     axes[1].title.set_text('ACF of absolute log-returns')
    #     axes[2].title.set_text('Leverage effect')
    #     plt.show()
    
    
    # x_real=np.log(df.iloc[1:,:].values/df.iloc[:-1,:].values)
    # x_fake=np.array([np.log(generated_data[i][1:]/generated_data[i][:-1]) for i in range(len(generated_data))])
    
    # comparison(x_fake,x_real)
    
    # #==========serial_ACF    
    # acf_x2, interval = acf(x=x2_returns.iloc[:,i],nlags=5,alpha=0.05)
    # acf_x1, interval = acf(x=x1_returns.iloc[:,i],nlags=5,alpha=0.05)
    # np.linalg.norm(acf_x1-acf_x2)
    
    
    
        
    
    
    # f = open("Label.txt",'a') #读取label.txt文件，没有则创建，‘a’表示再次写入时不覆盖之前的内容
    # f.write('i i i')
    # f.write('\n') #实现换行的功能
    # f.close()


from pathlib import Path
#main(h=4,e=1,g=2)
for i in [0.5,2,4,5]:
    for j in [1,2,3]:
        for k in [1,2,3]:
            results_path = Path('Hyperparameter_Tune/'+str([i,j,k]))
            if not results_path.exists():
                results_path.mkdir()
            main(i,j,k,path='Hyperparameter_Tune/'+str([i,j,k]))   #/content/drive/MyDrive/


